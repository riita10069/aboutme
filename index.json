[{"authors":["admin"],"categories":null,"content":"東工大情報通信系B3\n株式会社エブリーでReactとgolangを用いた開発と、新規事業MAMADAYSのメディア事業の立ち上げを担当した。株式会社Journeyでは分析チームを立ち上げ、MLモデル、MLOpsとDWH構築。またKPIダッシュボードの作成と企画立案分析など。\n株式会社TechDesignでは経営メンバー(CTO)として、2020年2月にジョイン。\n今の価値観を取り壊し、これからの常識に変えていく。\nそして、ITを用いて世界中の人々の暮らしを豊かにする。\nRyota is a third-student of Tokyo Institute of Technology and major in computer science.\nI like Computer Science and Business Intelligence.\nAnd Ryota Yamada is a Chief Technology Officer at Tech Design Inc.,\nPreviously, he improved essential problem solving and \u0026lsquo;create machine learning applications\u0026rsquo; at Journey Inc.,\nlearned \u0026lsquo;web programming\u0026rsquo; with golang and react.js at every inc. internship\nHe is highly skilled with problem solving, management strategy and GCP.\nHe wants to enrich human life with technology and bussiness.\nこれまでの開発経験を列挙する。\nI have the following skills.\nSkills  Web\nReact, golang(echo), Ruby(Rails) Infra\nGCE(Ansible), GKE(Knative, Istio, kustomize), CloudRun, nginx(sidecar), micro service Database\nMySQL, BigQuery, Redis, Datastore DevOps\ncircleCI, cloud build, Terraform, Ansible MLOps\njupyter, papermill, cloud compsoer, DWH, flask(pandas-gbq), Dataproc Collect Data\nfluentd, stackdriver, Pub/Sub, Dataflow(streaming), BigQuery Data processing\nSQL, pandas, spark(python), feature-tool(python) MLModel\nscikit-learn, xg-boost, AutoML, BigQueryML, Spark(MLlib) Business\nManager, MRD, PRD, Resourceful, SEO, GDN, KPI, OKR  Philosophy プログラミングを始めた時のこと\n成果を出すために取り組んでいること\n主体的な姿勢であること\n","date":1599330458,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1599330458,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://riita10069.github.io/aboutme/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/aboutme/authors/admin/","section":"authors","summary":"東工大情報通信系B3\n株式会社エブリーでReactとgolangを用いた開発と、新規事業MAMADAYSのメディア事業の立ち上げを担当した。株式会社Journeyでは分析チームを立ち上げ、MLモデル、MLOpsとDWH構築。またKPIダッシュボードの作成と企画立案分析など。\n株式会社TechDesignでは経営メンバー(CTO)として、2020年2月にジョイン。\n今の価値観を取り壊し、これからの常識に変えていく。\nそして、ITを用いて世界中の人々の暮らしを豊かにする。\nRyota is a third-student of Tokyo Institute of Technology and major in computer science.\nI like Computer Science and Business Intelligence.\nAnd Ryota Yamada is a Chief Technology Officer at Tech Design Inc.,\nPreviously, he improved essential problem solving and \u0026lsquo;create machine learning applications\u0026rsquo; at Journey Inc.,\nlearned \u0026lsquo;web programming\u0026rsquo; with golang and react.js at every inc. internship\nHe is highly skilled with problem solving, management strategy and GCP.","tags":null,"title":"Ryota Yamada","type":"authors"},{"authors":["Ryota Yamada"],"categories":["データ基盤"],"content":"はじめに みなさんこんにちは。りいたです。 自分のブログ初の技術的な記事の投稿なので、それに相応しいような僕が初めて自力でやったプロジェクトについてお話しします。 ちなみに特に最強でもなんでもないので、ご容赦ください。 DWHを作っている他の会社さんの参考になったり、データ分析をしているけれど、DataOps的な考え方をこれから導入しようと考えている人の参考になればいいなと思って書いています。 プログラミング素人の大学生のいってることなので、間違ってることたくさんあると思うので、Qiitaの方で指摘していただけたらありがたいです。\nちょうど一年前くらいです。大学2年の4月から、株式会社Journeyという会社でデータサイエンティストを務めています。\nプレシリーズAくらいの小さな会社なのですが、この会社がやっているミニッツというサービスは、後払いの旅行サイトです。 後払いという特徴を持っているために、支払いが正しく済まされないお客さんがあとを立たないという課題を抱えていました。 そこで、機械学習を用いた不正検知を実装して、回収率を上げていくというプロジェクトの責任者として、この会社にジョインすることになりました。 そこで、当然必要になってくるのが、DataPlatformです。 と言っても、この会社に入った頃の僕はプログラミング初めて4ヶ月くらいだった上にデータを扱った経験が一切なかったために、何もわからず、調べたり、社内のエンジニアに教えてもらいながら進めていったので、道はすごく険しかったです笑\n前提 データ基盤の３分類 + 1    分類 役割 主なツール     DataLake 行動ログやDBのデータを分析用にとりあえず置いておくところ。データソース（水源）から流れてきたデータをそのまま蓄える場所なのでレイク（湖） BigQuery, BigTable   DataWareHouse 事業のドメインに従って、再集計、加工したもの。基本的にはドメインにつながりのあるテーブルをjoinしたり、時差の修正をしたり。意味のあるものである必要はない。分析しやすいことが大事 BigQuery, Dataproc   DataMart SQLやPythonになれていないBiz向けにデータを加工・整理したものですね。完成品を取り揃えていることからマート（市場）と呼ぶらしいです。ここでは、GUIで必要なデータが取り出せることが重要事項と考えています。 BigQuery, metabase, dataprep   FeatureTable 機械学習の特徴量はここに追加していきます。BigQueryMLやAutoMLへの活用もしやすく、jupyterでのデータ加工も楽になります。 BigQuery, csv, bigtable    今後この4つの単語が出てくるので、役割を一旦抑えてください。 実際は、FeatureTableを除いた三つの分け方を、データ基盤の３分類と呼ぶことが多いと思います。 いわゆるDataOpsという界隈では、ゆずたそさんの提唱する進化的データモデリングはすごく有名だと思います。 https://www.amazon.co.jp/exec/obidos/ASIN/B087R6P8NZ/yuzutas0-22/ データサイエンティストや機械学習エンジニアのかたは、こちらの書籍は1度目を通して置いてもいいと思います。\nそもそもData Lakeがなかった この会社には、機械学習のモデルを作ってくれと言われて入ったのですが、現状はAIどころの騒ぎではなかったです。 MySQLに入っているデータはともかく、行動ログなんてそもそも残っていなかったんです笑\nなので、まずは、DataLakeを作るところから始める必要があります。\nDataLakeを作る プロダクトそのものは、AWSのEC2で動いていたので、\n Amazon Glue Amazon EMR Amazon Athena Amazon RedShift  など、データレイクのためのプロダクトはAWSにも十分揃っていると思います。\nまぁ、結論から言うと、わざわざGCPに持っていって、BigQueryを採用しました。 これも先に結果をいってしまうと、AthenaよりもBigQueryの方がメリットが強かったかなと思っています。\nBigQueryの利点は、\n 安い。数十GBくらいなら無料枠でやっていける。 使ってる人が多いので、文献が多い。  SQLでの分析を前提とするならAthenaとかRedShiftよりは、BigQueryは高速なので、開発体験もいいかなと思います。 EMRとか使うってなると、高速に動作すると思いますが、SQLって普遍的なスキルで書きやすいので、好き好んで、Spark使わなくて持ってなっていた部分もあります。\n当時は微妙だったのですが、今では、BigQueryMLがある程度使えるようになってきたことと、AutoMLがかなり楽にグラフの描画などを含めてやってくれることを考えると、BigQueryの外側に出る必要がどんどんなくなってきていると思っています。 ちゃんとモデルをつくり込むときは、JupyterでPython使って書いていたので、Spark使うことあまりなかったです。(pandas-gbqでBigQuery上のFeatures Tableからデータを取ってきています。) DataLakeを直接食わせるわけではないので、なんとかなっていましたが、教師データがTB以上になった時にはSparkが必要になってくると思うので、その辺は、現場によってかなって思っています。\nBigQueryにとりあえず、入れると決めたので、設計ができました。\n行動ログ nginx -\u0026gt; fluentd -\u0026gt; stackdriver -\u0026gt;bigquery\nMySQLのデータ slaveでdumpを作成して、それをGCSにあげるバッチを作成(これをyebisと命名) これをBigQueryに入れる。 このバッチはdigdagを使って1日に一回動くようにした。\nなんでembulk使わないの？ dumpが吐ける程度のデータ量しかなかったからです。 embulkってGCS経由のプアグインありましたっけ？直接入れるよりその方が安いんですよね。\nなんでcomposer使わないの？ AWSとGCPのセキュアコネクトするのがしんどかったんですよ。 特にAuroraにアクセスする方はシビアなので。\nfrom fluentd to bigquery 取りたいログをaccess.logに出力するために、Loggerを作る必要があります。 弊社はRailsを使っていたので、デフォルトのLoggerを活用したほうが賢いので、モンキーパッチをあてました。\nmodule Server module Logger class Formatter \u0026lt; ::Logger::Formatter include ActiveSupport::TaggedLogging::Formatter  Goを使っている場合がほとんどだと思いますが、Goではモンキーパッチがあてられないので、echoとかの既存のLoggerを使って、っていうのは厳しいですね。 僕は、github.com/TV4/logrus-stackdriver-formatterとgithub.com/plutov/echo-logrusを使って実装したことがあります。\npackage middleware type ( Log struct { Logger *logrus.Logger } ) func (l *Logs)getLogs(c echo.Context, next echo.HandlerFunc) error { logFields := map[string]interface{}{ \u0026quot;time_rfc3339\u0026quot;: time.Now().Format(time.RFC3339), \u0026quot;remote_ip\u0026quot;: c.RealIP(), \u0026quot;host\u0026quot;: req.Host, \u0026quot;uri\u0026quot;: req.RequestURI, \u0026quot;method\u0026quot;: req.Method, \u0026quot;path\u0026quot;: p, \u0026quot;referer\u0026quot;: req.Referer(), \u0026quot;user_agent\u0026quot;: req.UserAgent(), \u0026quot;status\u0026quot;: res.Status, \u0026quot;latency\u0026quot;: strconv.FormatInt(stop.Sub(start).Nanoseconds()/1000, 10), \u0026quot;latency_human\u0026quot;: stop.Sub(start).String(), \u0026quot;bytes_in\u0026quot;: bytesIn, \u0026quot;bytes_out\u0026quot;: strconv.FormatInt(res.Size, 10), \u0026quot;claims\u0026quot;: claims, } l.Logger.WithFields(logFields).Info() }  的な感じの実装をしました。 https://github.com/plutov/echo-logrus/blob/master/middleware.go#L152 を参照すればだいたいこんな感じかなってできました。\nfrom mysql to bigquery (yebis) MySQLからデータ取ってきて、頑張ってCVS作って、S3に上げておいてください。 今度はそれを取ってきて、BigQueryに上げています。\nload_job = dataset.load_job(table_id, file_path, skip_leading: 1, schema: schema) load_job.wait_until_done!  こんな感じの関数があるので、意外と簡単にできます。\ndigdagが定期実行してくれます。\ntimezone: Asia/Tokyo schedule: daily\u0026gt;: 04:00:00 _export: slack_webhook_url: https://hooks.slack.com/services/******** slack_webhook_channel: \u0026quot;#general\u0026quot; +setup: sh\u0026gt;: bundle install +run: sh\u0026gt;: ruby main.rb  データ基盤の３分類を始める。 解決すべき課題（Issue to be Solved）  特徴量を増やすのが大変すぎる  地道にSQLを大量に書かないといけない 書いたからといって、精度があがるとは限らない   データベースが汚すぎる 現状、欲しいデータがいつでもすぐに管理できているとは言えない  依頼、レスポンスまでの時間がある程度ある 一度に大量の依頼をしても返せない GUIで分析できたらそれはそれで便利では(metabase, dataprep)   KPIの管理がイマイチ  SQLを書いたはいいけど、ビジネス視点でクリティカルじゃない Dashboardは正しくルールを決めて管理するべきなのでは   クエリ管理ができてない  書いたクエリの厳密な定義がもはや記憶にない 書いたクエリにどんなものがあったか覚えていない    分析というのが、アドホックな検証という役割だけでなく 行動や属性から、ユーザーをクラスタリングし、 それによってどのようなクラスの属性にどの施策が刺さったのかを考える 仮説構築の手助けができるようなものを目指してく方がいい。\nというのが、当時のPRDに僕が書いたものです。 簡潔に言えば、BigQueryから頑張って引っ張ってくればいいというフェーズに限界がきていたんです。\n 同じようなデータセットをなんども作り直すのが大変すぎる ちょっとしたKPIの要件変更に対して、全てのクエリを修正しないといけない  という2点です。 批判とかではないですが、少なくとも弊社のビジネスも強くはないので、余分にKPI作りすぎというのもありました。 見ている数字の数が多すぎるし、定義が頻繁に変わる。 定義の変更は本当に厄介で、200ものクエリを全部読んで、またちょこっと変更なんてことをしないといけなかったので、DataWareHouseにKPIに関わるような重要な定義は外だしにしてあげて、DataMart側はFROM句でそこを参照するような形にしておかないとやってられないというのは強くありました。 後々課題をあげますが、こういった課題を、実感していないとDataOpsは厳しいと思います。なので、普段からそのデータセットでSQLを書いているデータ分析者でないSREの人とかがComposer導入できるからといって、データ基盤まで導入しても形骸化してしまうと思います。 実際この中間テーブルの考え方は、200行のクエリを30行にするくらいの威力のあるものでしたので、正しく運用できれば、価値のあるものになると思います。 また、KPI定義の変更に対しても非常に強いです。DWHを変えれば自然にDataMartも変わりますので。\n先に設計どーん Cloud Composer (bluemoon) Airflowのマネージドサービスですが、BigQuery Operatorが用意されていて、データ基盤の３分類やるならこの組み合わせが一番楽なような気がします。\nTerraformでCloud Composerを立てています。 というのも、composerって1日1回しか動かないのに、ずっと起きてるとお金もったいので、朝以外は落としています。 その時にTerraformだと管理しやすかったので、Terraformを使っています。\nterraform { backend \u0026quot;gcs\u0026quot; { bucket = \u0026quot;bluemoon-tf-state\u0026quot; path = \u0026quot;composer.tfstate\u0026quot; credentials = \u0026quot;account.json\u0026quot; } } provider \u0026quot;google\u0026quot; { credentials = file(\u0026quot;account.json\u0026quot;) project = \u0026quot;minute-journey\u0026quot; region = \u0026quot;asia-northeast1\u0026quot; } resource \u0026quot;google_composer_environment\u0026quot; \u0026quot;composer-environment\u0026quot; { name = \u0026quot;cloud-composer\u0026quot; project = \u0026quot;minute-journey\u0026quot; region = \u0026quot;us-central1\u0026quot; # 当時はdataprocとかdataflowがasia対応してなかったので config { node_count = 3 node_config { zone = \u0026quot;us-central1-a\u0026quot; machine_type = \u0026quot;n1-standard-1\u0026quot; disk_size_gb = 20 } software_config { airflow_config_overrides = { core-dag_concurrency = 20 } } } }  ディレクトリ構成 . ├── tf... ├── data_source_a.py ├── data_source_b.py └── data_source_c.py └── sql ├── data_source_a.sql ├── data_source_b.sql └── data_source_c.sql  import datetime import codecs from airflow import models from airflow.contrib.operators import bigquery_operator from airflow.operators import bash_operator with codecs.open('/home/airflow/gcs/plugins/sql/data_source_a.sql', 'r', 'utf-8') as f: query = f.read() DAG_NAME = 'base_query' default_dag_args = { 'start_date': datetime.datetime(2018, 1, 1), 'email_on_failure': False, 'email_on_retry': False, 'retries': 1, 'retry_delay': datetime.timedelta(minutes=10), 'project_id': \u0026quot;minute-journey\u0026quot; } with models.DAG( dag_id=DAG_NAME, schedule_interval=\u0026quot;@daily\u0026quot;, default_args=default_dag_args) as dag: start = bash_operator.BashOperator( task_id='start', bash_command='echo Start Workflow' ) bq_to_bq = bigquery_operator.BigQueryOperator( task_id='query', write_disposition='WRITE_TRUNCATE', create_disposition='CREATE_IF_NEEDED', allow_large_results=True, sql=query, use_legacy_sql=False, destination_dataset_table='minute-journey.data_ware_house.data_source_a' ) start \u0026gt;\u0026gt; bq_to_bq  実行方法 gcloud composer environments storage dags import --location us-central1 --environment cloud-composer --source base_query.py gcloud composer environments storage plugins import --location us-central1 --environment cloud-composer --source sql/  定期実行 先ほども言いましたが、ずっと立ってると3台分時間課金されて勿体無いので、朝以外は落ちてるようにしています。 CircleCIのcron使って、午前8:45にterraform applyして、9:30にterraform destroyしています。 キューイングとかもできるような感じはありますが、キューイングされる先にお金かかるような気がして、やってないです。\nまぁ、やるとしたらCloud runにTerraform環境んで、composerのbash operatorでそれ叩くかなぁって感じですが、今の所30分はかからないってわかってるので、まぁいいかなってなっています笑\nデータ量が増えてきてできなくなったら手作業で10:00にしておけばいいかなと。。。w この規模の会社だとデータ基盤なんて1日古くても大した問題ではないので。\nversion: 2 jobs: create: working_directory: ~/bluemoon docker: - image: hashicorp/terraform:light steps: - checkout - run: name: Slack notification command: sh start_slack.sh working_directory: . - run: name: Init terraform command: terraform init working_directory: tf/cloud-composer - run: name: Apply terraform command: terraform apply -auto-approve working_directory: tf/cloud-composer destroy: working_directory: ~/bluemoon docker: - image: hashicorp/terraform:light steps: - checkout - run: name: Slack notification command: sh end_slack.sh working_directory: . - run: name: Init terraform command: terraform init working_directory: tf/cloud-composer - run: name: Destroy terraform command: terraform destroy -auto-approve working_directory: tf/cloud-composer deploy: working_directory: ~/bluemoon docker: - image: google/cloud-sdk steps: - checkout - run: name: initialize gcloud command: | gcloud auth activate-service-account --key-file=tf/cloud-composer/account.json gcloud --quiet config set project minute-journey - run: name: import sql files command: gcloud composer environments storage plugins import --location us-central1 --environment cloud-composer --source sql/ - run: name: import dag files command: gcloud composer environments storage dags import --location us-central1 --environment cloud-composer --source base_query.py workflows: version: 2 start_workflow: triggers: - schedule: cron: \u0026quot;45 23 * * *\u0026quot; filters: branches: only: - master jobs: - create - deploy: requires: - create filters: branches: only: master end_workflow: triggers: - schedule: cron: \u0026quot;30 0 * * *\u0026quot; filters: branches: only: - master jobs: - destroy  MLOps (hinano) 不正かどうかの確率を計算して返すAPIはhinanoと読んでいます。 GCSからモデルのpickleファイルを取ってきて、APIコールで特徴量をもらってそれを計算して返すって感じです。\ndef load_model(): project_name = \u0026quot;minute-journey\u0026quot; storage_client = storage.Client(project_name) bucket_name = 'hinano' bucket = storage_client.get_bucket(bucket_name) #アップロードしたファイルをダウンロード blob_download = bucket.get_blob('xgb_reservation_model.pickle') loaded_model = pickle.loads(blob_download.download_as_string()) return loaded_model def score_predict(clf, reservation_log): y_pred_proba = clf.predict_proba(reservation_log) y_pred = clf.predict(reservation_log) return y_pred[0], y_pred_proba[0][0]  APIにはFlask使っています。 結構作るの時間かかりましたが、正直大したことはやっていないです。 Service層にあるのがこれくらいなので、この辺参考にすれば簡単にできるはずです。\nDWHの運用で出てきた問題点 DataLakeからDataMartを直接作ることを厳しく禁止しすぎた。 DataWareHouseが肥大化しすぎてドキュメント管理がしんどくなってきた。 同じようなDataWareHouseができてきて、結局最初の課題に戻ってしまう。 データモデリングに関しては、他の分野ほど知見が溜まっていないのもあって、ある程度リファクタリング前提で考えていいと思っています。 というのも、Atomic DesignやDDDのような強力な型があれば乗っかってもいいですが、現状この分野にはそれがないと思うので、 頻繁にジョインするテーブルがわかったらそれをくっつけて、サブクエリを全て置き換えるというような運用をしてもいいと思います。 僕らの運用だと1Lake1Houseが多すぎたので、それは本質的ではないと思います。\n謎のDWH データの定義って難しいですよね。 それってuniqueなの？こういうパターンは別で数えるの？それって本質なの？週別なの？土曜から日曜にかけて宿泊した人はどっち扱いなの？ 定義しなきゃいけないことって思っていることの何倍も多いです。 そういう曖昧な定義を許していると、なんとなく書いたクエリがそうだったからでDWHに曖昧の負債が溜まっていきます。 本質的なプロダクトの改善に役立てることを考えるなら、データの定義を厳密にすることが重要だと思います。厳密な定義のデータが欲しくなるくらいには、そもそも仮説が練りこまれていないと分析する価値がないとも言えますし、データの定義を考えることは有意義だと思います。\n僕は、ディメンショナルモデリング的な考え方にはあまり共感していないというか、そこまでの規模のプロダクトではなかったのかもしれないですが、データの定義に関しては、厳密なものを作り込んでおく必要があるので、ある程度、ファクトとディメンションを分離して、スタースキーマを作成したりすることで、精査されるかもしれません。 ただ、重要なのはいつでも本質から外れないことです。\n","date":1599330458,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599330458,"objectID":"9032063289fcf5648aa8725d2ede564b","permalink":"https://riita10069.github.io/aboutme/post/data-ops/","publishdate":"2020-09-06T03:27:38+09:00","relpermalink":"/aboutme/post/data-ops/","section":"post","summary":"はじめに みなさんこんにちは。りいたです。 自分のブログ初の技術的な記事の投稿なので、それに相応しいような僕が初めて自力でやったプロジェクトについてお話しします。 ちなみに特に最強でもなんでもないので、ご容赦ください。 DWHを作っている他の会社さんの参考になったり、データ分析をしているけれど、DataOps的な考え方をこれから導入しようと考えている人の参考になればいいなと思って書いています。 プログラミング素人の大学生のいってることなので、間違ってることたくさんあると思うので、Qiitaの方で指摘していただけたらありがたいです。\nちょうど一年前くらいです。大学2年の4月から、株式会社Journeyという会社でデータサイエンティストを務めています。\nプレシリーズAくらいの小さな会社なのですが、この会社がやっているミニッツというサービスは、後払いの旅行サイトです。 後払いという特徴を持っているために、支払いが正しく済まされないお客さんがあとを立たないという課題を抱えていました。 そこで、機械学習を用いた不正検知を実装して、回収率を上げていくというプロジェクトの責任者として、この会社にジョインすることになりました。 そこで、当然必要になってくるのが、DataPlatformです。 と言っても、この会社に入った頃の僕はプログラミング初めて4ヶ月くらいだった上にデータを扱った経験が一切なかったために、何もわからず、調べたり、社内のエンジニアに教えてもらいながら進めていったので、道はすごく険しかったです笑\n前提 データ基盤の３分類 + 1    分類 役割 主なツール     DataLake 行動ログやDBのデータを分析用にとりあえず置いておくところ。データソース（水源）から流れてきたデータをそのまま蓄える場所なのでレイク（湖） BigQuery, BigTable   DataWareHouse 事業のドメインに従って、再集計、加工したもの。基本的にはドメインにつながりのあるテーブルをjoinしたり、時差の修正をしたり。意味のあるものである必要はない。分析しやすいことが大事 BigQuery, Dataproc   DataMart SQLやPythonになれていないBiz向けにデータを加工・整理したものですね。完成品を取り揃えていることからマート（市場）と呼ぶらしいです。ここでは、GUIで必要なデータが取り出せることが重要事項と考えています。 BigQuery, metabase, dataprep   FeatureTable 機械学習の特徴量はここに追加していきます。BigQueryMLやAutoMLへの活用もしやすく、jupyterでのデータ加工も楽になります。 BigQuery, csv, bigtable    今後この4つの単語が出てくるので、役割を一旦抑えてください。 実際は、FeatureTableを除いた三つの分け方を、データ基盤の３分類と呼ぶことが多いと思います。 いわゆるDataOpsという界隈では、ゆずたそさんの提唱する進化的データモデリングはすごく有名だと思います。 https://www.amazon.co.jp/exec/obidos/ASIN/B087R6P8NZ/yuzutas0-22/ データサイエンティストや機械学習エンジニアのかたは、こちらの書籍は1度目を通して置いてもいいと思います。\nそもそもData Lakeがなかった この会社には、機械学習のモデルを作ってくれと言われて入ったのですが、現状はAIどころの騒ぎではなかったです。 MySQLに入っているデータはともかく、行動ログなんてそもそも残っていなかったんです笑\nなので、まずは、DataLakeを作るところから始める必要があります。\nDataLakeを作る プロダクトそのものは、AWSのEC2で動いていたので、\n Amazon Glue Amazon EMR Amazon Athena Amazon RedShift  など、データレイクのためのプロダクトはAWSにも十分揃っていると思います。","tags":["DataOps","DWH"],"title":"【DataOps】大学2年生の時に作った最強のDataPlatform","type":"post"},{"authors":["Ryota Yamada"],"categories":["幸福論"],"content":"死ぬために生きること、幸せに生きること あなたは幸せに生きたい。何か面白いことをして生きていたいと思ったことはありますか？\nぼくは、大学に入学するまでは、たったの一度も思ったことなかったんですけど、大学に入学してからはそれをすごく強く思うようになりました。 幸せに生きるってなんだろう。何をしている時が幸せなんだろうといろいろ考えました。 ぼくの親や友達は、\n  「安定していて食いっぱぐれがない」\n  「一生楽して稼げる」\n  「高い給料が欲しい」 とかそんなことばかり言っています。 でも、\n  お金持ちって幸せかな？\n  大企業のエリートは幸せかな？\n  楽して稼げることが幸せかな？\n  って不思議に思いました。皆さんはそう思いません？ みんなはなんでそんなにも安定を好むのか。 寿命を全うすることが目的みたいになっていて、まさに**「死ぬために生きている」**という状態なのかなと思いました。\n「幸せに生きること」と「死ぬために生きること」というのは全く別だと思うんです。 幸せに生きるために大切なことってなんだと思いますか？ ぼくは、なんで大学に入ってから、幸せに生きたいって思うようになったんだと思いますか？\n幸せな人生にする上で大切なことは 「苦労しても乗り越えたいと感じられる目標を持つこと」です。\nぼくは大学に入学して、なんで幸せになりたいと思うようになったかというと、大学入学前と大学入学後の人生を比較して、入学後の人生っていい人生じゃなかったんです。 良い人生じゃないというのは、**「困難に挑戦していない」**ということです。\nなぜかというと、 ぼくは、東京工業大学という、日本の工学部で2番目くらいの偏差値の大学に通っています。 世間的に言われる「高学歴」というやつです。 それは何を表すかというと、 ぼくの中で「想像可能な人生」に一気に変わってしまったんです。\n大変だけど、年に8回ある定期試験の勉強をして、なんだかんだ単位は取って。 院試の勉強もぼちぼちやれば、大学院にも進めて、研究室の推薦使って就職して、まぁまぁの大企業に入って。 30くらいになったらちょうどその時に付き合っていた女性と多分結婚するんだろうな。 大企業のサラリーマンになれば、毎日誰にでもできるような作業しかしない。 作業すればお金もらえるから別に忙しかったとしても、淡々と作業を進めることが想像できるんですよ。これからこうなるだろうな。って。 それでは、生きてる意味を見出せてないというか。\nだけどぼくはその本質に気が付けないで、「幸せに生きたいな〜」って思うようになったんだと思います。 高校生までのぼくは、どうも負けず嫌いがすぎて、部活もゲームも勉強も気が狂ったみたいに努力していたので、辛いと思うことはあっても、つまらないと感じたことはなかったんですけどね。 大学に入ってからは、こういうわけで毎日が物足りないと感じるようになりました。\nでも、プログラミングに全力で取り組み始めてからは、また日々がめっちゃ楽しくなりました。 「これまでやったことないことに全力で取り組むこと。」によって、これから起こることが予測できなくなるので、めっちゃくちゃ楽しくてワクワクするんです。 でも、人って予測できない未来をなぜか毛嫌いするんですよ。 先生とか親とか、周りの友達は、 「そういうのはやめろ」 「一番大切なのは、真面目に生きること」 みたいなことばかり言ってきます。 でも、ぼくが責任を持って断言します。 それは大きな間違いで、困難に挑戦することを習慣化した方が幸せなのです。 少し厳しいことをいうようですが、 世の中で「真面目」と言われる人は、親や先生に言われたことを言われた通りにやります。 親や先生にとっては、自分の言うことをよく聞いてくれるからとても褒めます。 それに、言われたことを言われた通りにやるのって楽なんですよ。無駄な摩擦を生まないし、自分で考えなくていいし、何より親や先生に褒められますからね。 でも、言われたことを言われた通りにやる「真面目人間」というのは、(自分ではない)誰かの作った答えを「ただなぞってる」だけなのです。 でも、自分の価値観を持たず他人の価値観で生きているから一生幸せになれないんです。 そもそも自分の信念すら存在しないのです。 難しくて自分にはできないなと思ったとしても、とにかく自分がやりたいと感じたことをやる。 もう「真面目人間」はやめましょう。 もう他人に認められるための人生はやめましょう。 人に褒められるための行動もやめましょう。 あなたの感じたその感性で、達成したいと思った目標のために行動しましょう。\nそして、僕たちは非常に幸運な時代に生まれました。 僕らは閉塞した時代に生まれた運の悪い子どもたちではありません。 自由の時代です。自己実現の時代です。 近年のテクノロジーの進化によって、不可能は可能になります。 今まさに、これまでの古臭い手段がクリエイティブで画期的な仕組みに置き換わっている真っ最中です。 親や先生のせいにして、何もせずに、世界が変わってゆくのを指をくわえて見ているだけになるか、 それともあなたが世界を変える側になるか。 さぁ、どっちを選びますか？\nぼくはみなさんに必ずしもプログラミングをやれって言ってるわけではないです。 本当になんでもいいんです。ゲームでも、仕事でも、部活動でも。 ただ、これまでやったことないことに「全力で」取り組んでください。 困難に挑戦して、リスクを背負ってください。 まじで毎日ワクワクできます。その瞬間から幸せに生きれます。 まだここにない未来を見つけてください。 そうすれば、 **「多大な苦労を伴ってもそこに到達してみたい」**という風に心から感じられる目標がきっとみつかります。 これが最も人間を動かす大きな力になります。 このように感じられる目標を持った瞬間にあなたの人生は成功します。 それが人間の感じる本当の幸せです。\n(とはいえ、周りの雑音を気のしないのって難しいので、コツを教えます。 安定した大企業に就職しろとか、楽して稼げるようになれとか、ぼくも親に強く強く言われています。 でも、「安定」とか、「楽して稼ぐ」とか、そういった発想って器が小さいなぁと思いません？ この人は、稼いだ金の額で評価されいんだ。小さい人だなぁ。 「ぼくは楽することにも、稼ぐことにも興味がない。」と心の中で思っておけば他人の雑音なんて全く気にならなくなります。決して反抗してはダメです。価値観の問題なので。)\n何か新しいことに全力で取り組む 「困難に挑戦する」いうのは、ちょっと言葉は大げさですが本当に些細なことからでいいんです。 要するに「これまでやったことないことは気になったらなんでもやってみる。」 ってことなんです。 そのなかから、本当に全力で取り組みたいものを決めればいいんです。\n例えば、\n 今日話したちょっとかわいいなって思った女の子(かっこいいなって思った男の子)に勇気を持って、「今日はありがとう」ってラインを送ること 好きな人をご飯に誘ってみること 不安だけれど、海外に一人で短期留学に行ってみること 何か新しいコミュニティに入ること 値段は高いけど気になっていたお店に入ってみること。 プログラミングを始めること 身の丈に合わない難関大学を目指してみること  のように、最初は小さいものなのです。その中から全力でやるものが決まるんです。\nこれまでだったら、「勇気を持てなくてやらないな」と思うことをちょっと視点を変えてみて、今日からはやってみる。 ほんの些細なことであっても、それだけで、格段に人生は豊かになります。\nもちろん、それだけではありません。 仕事の場でも、いろいろなことが**「困難に挑戦すること」**としてあげられます。 その中の１つに **「これまでの先輩方がやってきたオペレーションを一新して新しいことに取り組む」**ということも含まれると思います。 常々ぼくは、**「言われたことをしっかりやると人生うまくいく」というのは大きな間違え**だと言っています。 本当に会社にとって大きな利益を生み出すのは、 「これまでなかった仕事を0から生み出すこと」 「誰も取り組まなかった課題に取り組むこと」 だと思っているからです。 だから、「現状に課題を感じて、これまでやっていなかったことをやる」というのは、最高の成果を提供する手段です。\nしかし、これまであったものを否定して新しいことを始めると、 それがどんなに素晴らしいことであっても必ず非難されます。必要ないといわれます。 これまであったものを否定することは、そのようなオペレーションを組んでいた先輩方を否定することになりますから。 当然、新しいことを始めるには自分一人では何もできません。 周りの人の協力がとにかく1番大切です。 だから、周りを巻き込んで行動していく行動力と主体性が本当に重要です。 そして、その時に大切なのは、**「なんでそれをやるのか」**について繰り返し丁寧に説明することです。\nぼくは、文化祭の実行委員会で、\n TIPS in Pocket という参加団体が工大祭の当日に必要な情報がA3両面にまとまったとても便利な資料を、発案から作成までやりきりました。 これまで、模擬店企画の参加申請を紙面で行なっていましたが、ぼくが完全フォーム化を実現しました。  この時もそうでした。 何か新しいことを始めようとすれば、必ず非難されます。ぼくがこの２つのことを始めた時も非常に多くの人に非難されました。 時には人格否定もされましたし、反省では長文での攻撃も受けました。それでもぼくは、これによって誰のどんな課題が解決するのかについて語り続けました。ぼくの作りたいビジョンを伝えました。その結果、手のひらを返したように多くの人が協力してくれました。 この文化祭の実行委員会の活動で気づきました。 チームにおいて大切なことは、「Why」を語ること。 その仕事の価値がわかれば、お金が動かなくても全力で協力してくれる人がいます。 そして、ぼく一人では何もできないけれど、多くのステークホルダーが協力すれば、ずっと不可能と言われてきたことが可能になると。 これは、ぼくがきっかけで、これまでの価値観を覆し、これからの常識を作り出した例です。 この文化祭の実行委員会での経験は一生忘れないぼくの思い出です。\n結局、「これまでやったことないことに全力で取り組むこと」は幸せになるためのきっかけを提供してくれているんです。\n ぼくは文化祭の実行委員会で当時二年生局長という役職で活躍していたひとつ上の先輩に憧れて全力で文化祭の改善に取り組みました。  これらがきっかけで、ぼくの人生は激変しました。とても豊かになりました。 別にそんなことでもいいんです。 きっかけは本当になんでもいいんです。 ただ、何かを本気でやることが大事なんです。\nあなたにとってのそれは、受験勉強かもしれないし恋愛かもしれないし仕事かもしれません。 ただ何かに本気で打ち込むこと。挑戦する姿勢であり続けることが、一生の生き甲斐を見つけるきっかけになります。 これまでの価値観が大きく変わるきっかけになります。 少なくとも僕はこのように、文化祭の実行委員会に所属して人生が大きく変わりました。今の僕の価値観を作ってくれたのは間違いなく工大祭実行委員会での活動です。\n僕は、まだ20歳でこれから先の人生のことについて、わからないことだらけです。 今ここに書いた理念が変わる日がすぐに来るかもしれません。 むしろ、自分の理念や価値観が変わることを楽しんで生きていきたいと思っています。 だから、是非みなさんも何かこれまでやったことのないことに全力で取り組んで欲しいと願っています。\n","date":1597657161,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597657161,"objectID":"2891ef7ac669613ed704a6ee4440ca95","permalink":"https://riita10069.github.io/aboutme/post/first-post/","publishdate":"2020-08-17T18:39:21+09:00","relpermalink":"/aboutme/post/first-post/","section":"post","summary":"どんなことに取り組む場合でも、主体的な姿勢であることが最も大切","tags":["幸福論"],"title":"主体的な姿勢であることは幸せであること","type":"post"}]